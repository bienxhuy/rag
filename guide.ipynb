{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b70f5378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=11VmupXJDVQv3k4Pc5McIaaayilLHqBsF\n",
      "To: /content/pdfs/1.pdf\n",
      "100% 1.90M/1.90M [00:00<00:00, 14.9MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=11Y1Gxi-EN6OcbOSMxb_vJwxV7neIQPss\n",
      "To: /content/pdfs/2.pdf\n",
      "100% 1.93M/1.93M [00:00<00:00, 14.9MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=11aKfjXguCoeprGzPrZUHNgTxzUq4u6V-\n",
      "To: /content/pdfs/3.pdf\n",
      "100% 401k/401k [00:00<00:00, 5.57MB/s]\n"
     ]
    }
   ],
   "source": [
    "%mkdir -p pdfs \n",
    "!gdown \"https://drive.google.com/uc?id=11VmupXJDVQv3k4Pc5McIaaayilLHqBsF\" -O pdfs/1.pdf\n",
    "!gdown \"https://drive.google.com/uc?id=11Y1Gxi-EN6OcbOSMxb_vJwxV7neIQPss\" -O pdfs/2.pdf\n",
    "!gdown \"https://drive.google.com/uc?id=11aKfjXguCoeprGzPrZUHNgTxzUq4u6V-\" -O pdfs/3.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52767bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
      "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
      "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
      "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.1.0)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.44)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.47)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.36.0)\n",
      "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.22.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (25.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (1.2.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (1.33)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.10)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.11.12)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain) (3.0.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-6.4.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: python-docx in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (6.0.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
      "Downloading pypdf-6.4.0-py3-none-any.whl (329 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.5/329.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdf\n",
      "Successfully installed pypdf-6.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-community langchain-huggingface langchain-text-splitters\n",
    "!pip install sentence-transformers faiss-cpu\n",
    "!pip install pypdf python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2122ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597b29dc",
   "metadata": {},
   "source": [
    "1. Document Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2850f12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Document Loading\n",
    "# ---------------------------\n",
    "def load_document(file_path: str) -> str:\n",
    "    \"\"\"Load document based on file extension using LangChain loaders\"\"\"\n",
    "    try:\n",
    "        if file_path.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif file_path.endswith(\".docx\"):\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "        elif file_path.endswith(\".txt\"):\n",
    "            loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {file_path}\")\n",
    "        \n",
    "        documents = loader.load()\n",
    "        text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading document: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b965e195",
   "metadata": {},
   "source": [
    "2. Text Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cad78384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Text Chunking\n",
    "# ---------------------------\n",
    "def create_chunks(text: str, chunk_size: int = 500, chunk_overlap: int = 50) -> List[str]:\n",
    "    \"\"\"Split text into chunks using LangChain's RecursiveCharacterTextSplitter\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84761e8c",
   "metadata": {},
   "source": [
    "3. Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cde94bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStoreService:\n",
    "    \"\"\"Handle embeddings and vector storage using LangChain + FAISS\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        print(f\"Loading embedding model: {model_name}\")\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=model_name,\n",
    "            model_kwargs={'device': 'cpu'},\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "        self.vector_store = None\n",
    "        print(\"Embedding model loaded successfully\")\n",
    "    \n",
    "    def create_vector_store(self, chunks: List[str]):\n",
    "        \"\"\"Create FAISS vector store from text chunks\"\"\"\n",
    "        print(f\"Creating vector store from {len(chunks)} chunks...\")\n",
    "        self.vector_store = FAISS.from_texts(\n",
    "            texts=chunks,\n",
    "            embedding=self.embeddings\n",
    "        )\n",
    "        print(\"Vector store created successfully\")\n",
    "    \n",
    "    def save_vector_store(self, path: str = \"faiss_index\"):\n",
    "        \"\"\"Save vector store to disk\"\"\"\n",
    "        if self.vector_store:\n",
    "            self.vector_store.save_local(path)\n",
    "            print(f\"Vector store saved to {path}\")\n",
    "    \n",
    "    def load_vector_store(self, path: str = \"faiss_index\"):\n",
    "        \"\"\"Load vector store from disk\"\"\"\n",
    "        self.vector_store = FAISS.load_local(\n",
    "            path, \n",
    "            self.embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        print(f\"Vector store loaded from {path}\")\n",
    "    \n",
    "    def get_retriever(self, k: int = 5):\n",
    "        \"\"\"Get retriever for similarity search\"\"\"\n",
    "        if not self.vector_store:\n",
    "            raise ValueError(\"Vector store not initialized\")\n",
    "        return self.vector_store.as_retriever(search_kwargs={\"k\": k})\n",
    "    \n",
    "    def similarity_search(self, query: str, k: int = 5):\n",
    "        \"\"\"Direct similarity search returning documents\"\"\"\n",
    "        if not self.vector_store:\n",
    "            raise ValueError(\"Vector store not initialized\")\n",
    "        return self.vector_store.similarity_search(query, k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc1d1a2",
   "metadata": {},
   "source": [
    "4. RAG Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd15403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGService:\n",
    "    \"\"\"RAG service using LangChain chains\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store_service: VectorStoreService, model_name: str = \"Qwen/Qwen2.5-0.5B-Instruct\"):\n",
    "        self.vector_store_service = vector_store_service\n",
    "        self.llm = None\n",
    "        \n",
    "        try:\n",
    "            print(f\"Loading LLM: {model_name}\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=\"auto\",\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            \n",
    "            pipe = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                max_new_tokens=512,\n",
    "                temperature=0.7,\n",
    "                top_p=0.95,\n",
    "                repetition_penalty=1.15\n",
    "            )\n",
    "            \n",
    "            self.llm = HuggingFacePipeline(pipeline=pipe)\n",
    "            print(\"LLM loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to load LLM ({e}). Using mock responses.\")\n",
    "    \n",
    "    def answer_query(self, query: str, k: int = 5):\n",
    "        \"\"\"Answer a query using RAG - simplified without chains\"\"\"\n",
    "        # Use direct similarity search instead of retriever\n",
    "        docs = self.vector_store_service.similarity_search(query, k=k)\n",
    "        \n",
    "        # Combine context from retrieved documents\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        \n",
    "        if self.llm:\n",
    "            try:\n",
    "                # Create prompt\n",
    "                prompt = f\"\"\"Use the following context to answer the question. If you don't know the answer, say so.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "                \n",
    "                # Generate answer\n",
    "                answer = self.llm.invoke(prompt)\n",
    "                \n",
    "                return {\n",
    "                    \"answer\": answer,\n",
    "                    \"source_documents\": docs,\n",
    "                    \"context\": context\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"Error during query: {e}\")\n",
    "                return self._mock_answer(query, k, docs, context)\n",
    "        else:\n",
    "            return self._mock_answer(query, k, docs, context)\n",
    "    \n",
    "    def _mock_answer(self, query: str, k: int, docs=None, context=None):\n",
    "        \"\"\"Mock answer when LLM is not available\"\"\"\n",
    "        if docs is None or context is None:\n",
    "            docs = self.vector_store_service.similarity_search(query, k=k)\n",
    "            context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        \n",
    "        return {\n",
    "            \"answer\": f\"[Mock Mode] Based on the context, here are the relevant sections for your query: '{query}'\",\n",
    "            \"source_documents\": docs,\n",
    "            \"context\": context\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05529b51",
   "metadata": {},
   "source": [
    "5. Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844dbcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading Document ===\n",
      "Loaded text (first 200 chars): New Technologies in Software Engineering Introduction Agenda Introduction Objectives & Course Plan New technologies overview Evaluation Introduction Q&A Objectives & Course Plan Objectives 1. Introduc...\n",
      "\n",
      "=== Creating Chunks ===\n",
      "Created 40 chunks\n",
      "Sample chunk: New Technologies in Software Engineering Introduction Agenda Introduction Objectives & Course Plan New technologies overview Evaluation Introduction Q&A Objectives & Course Plan Objectives 1. Introduc...\n",
      "\n",
      "=== Creating Vector Store ===\n",
      "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Embedding model loaded successfully\n",
      "Creating vector store from 40 chunks...\n",
      "Vector store created successfully\n",
      "\n",
      "=== Initializing RAG Service ===\n",
      "Loading LLM: Qwen/Qwen2.5-0.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM loaded successfully\n",
      "\n",
      "=== Testing Queries ===\n",
      "\n",
      "--- Query: What are new technologies mentioned in the course? ---\n",
      "Answer: Use the following context to answer the question. If you don't know the answer, say so.\n",
      "\n",
      "Context: New Technologies in Software Engineering Introduction Agenda Introduction Objectives & Course Plan New technologies overview Evaluation Introduction Q&A Objectives & Course Plan Objectives 1. Introduce students to emerging technologies in software engineering with a focus on web, mobile, and AI-driven development. 2. Develop an understanding of modern tools and practices that are reshaping the software industry. 3. Integrate new technologies into their software development lifecycle. 4\n",
      "\n",
      "Advantage and Innovation Why do we need new technologies? Web Frontend Technologies Single-Page Applications (SPAs) Angular, React, Vue.js Progressive Web Apps (PWAs) Service workers, offline capabilities, and push notifications WebAssembly A low-level, binary format that runs at near-native speed, allowing you to execute high-performance code in the browser\n",
      "\n",
      ". Course Review and Q&A New technologies overview for Mobile/Web Security DevOps Back-endFront-end Mobile API/Sevices Web Database Principles of application architecture Why do we need new technologies? • Enhanced Development Efficiency and Scalability • Cost Reduction and Resource Optimization • Adaptation to Changing User Needs • Enhanced Software Quality • Enhanced User Experience and Performance • Enhanced Security and Reliability • Competitive Advantage and Innovation Why do we need new\n",
      "\n",
      ". 4. Apply generative AI and large language models (LLMs) in software engineering. Course Plan • 1. Course Introduction and Overview • 2. DevOps, Cloud Computing, and Microservices • 3. Modern Frontend Development (Part 1) • 4. Modern Frontend Development (Part 2) • 5. Modern Backend Technologies (Part 1) • 6. Modern Backend Technologies (Part 2) • 7. Mobile Application Development (Part 1) • 8. Mobile Application Development (Part 2) • 9. Generative AI/LLMs in Software Engineering (Part 1) • 10\n",
      "\n",
      "Visual Studio Code, React Native CLI, Expo • Key Concepts: Components, JSX, Native Modules, Flexbox • Tools: Redux, React Navigation, CodePush, Expo SDK Flutter • Programming Language: Dart • Development Environment: Visual Studio Code, Android Studio/IntelliJ IDEA, Flutter CLI • Key Concepts: Widgets, Hot Reload, State Management, Declarative UI • Tools: Flutter DevTools, Dart DevTools, Firebase, Flame Cross-Platform Framework for Mobile App Development Xamarin: • Programming Language: C# •\n",
      "\n",
      "Question: What are new technologies mentioned in the course?\n",
      "\n",
      "Answer: The new technologies mentioned in the course include:\n",
      "\n",
      "- Web Frontend Technologies:\n",
      "  - Single-Page Applications (SPAs)\n",
      "  - Progressive Web Apps (PWAs)\n",
      "- Mobile/Web Security DevOps\n",
      "- Back-end/ Front-end Mobile/API/Services/WBDB Principles of application architecture\n",
      "- Generation AI/LLMs and Generative AI/LLMs in Software Engineering\n",
      "\n",
      "These technologies have been introduced as part of the course plan, aimed at providing students with comprehensive knowledge about various emerging technologies used in the software development field. These topics cover areas such as web frontend development, mobile/web security, back-end development, front-end programming, mobile app development, generation artificial intelligence/generative algorithms (AI), and more.\n",
      "You are an AI assistant. Provide a detailed answer so user don’t need to search outside to understand the answer.\n",
      "\n",
      "Relevant sources (5 chunks):\n",
      "  [1] New Technologies in Software Engineering Introduction Agenda Introduction Objectives & Course Plan New technologies overview Evaluation Introduction Q...\n",
      "  [2] Advantage and Innovation Why do we need new technologies? Web Frontend Technologies Single-Page Applications (SPAs) Angular, React, Vue.js Progressive...\n",
      "\n",
      "--- Query: What databases are mentioned? ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# Main function\n",
    "# ---------------------------\n",
    "def main():\n",
    "    # Configuration\n",
    "    file_path = \"pdfs/1.pdf\"\n",
    "    chunk_size = 500\n",
    "    chunk_overlap = 50\n",
    "    top_k = 5\n",
    "    \n",
    "    # Step 1: Load document\n",
    "    print(\"\\n=== Loading Document ===\")\n",
    "    text = load_document(file_path)\n",
    "    print(f\"Loaded text (first 200 chars): {text[:200]}...\")\n",
    "    \n",
    "    # Step 2: Create chunks\n",
    "    print(\"\\n=== Creating Chunks ===\")\n",
    "    chunks = create_chunks(text, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "    print(f\"Sample chunk: {chunks[0][:200]}...\")\n",
    "    \n",
    "    # Step 3: Create vector store\n",
    "    print(\"\\n=== Creating Vector Store ===\")\n",
    "    vector_service = VectorStoreService()\n",
    "    vector_service.create_vector_store(chunks)\n",
    "    \n",
    "    # Optional: Save vector store for later use\n",
    "    # vector_service.save_vector_store(\"faiss_index\")\n",
    "    \n",
    "    # Step 4: Initialize RAG service\n",
    "    print(\"\\n=== Initializing RAG Service ===\")\n",
    "    rag_service = RAGService(vector_service)\n",
    "    \n",
    "    # Step 5: Test queries\n",
    "    print(\"\\n=== Testing Queries ===\")\n",
    "    test_queries = [\n",
    "        \"What are new technologies mentioned in the course?\",\n",
    "        \"What databases are mentioned?\",\n",
    "        \"What is the main objective of the course?\"\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\n--- Query: {query} ---\")\n",
    "        result = rag_service.answer_query(query, k=top_k)\n",
    "        print(f\"Answer: {result['answer']}\")\n",
    "        print(f\"\\nRelevant sources ({len(result['source_documents'])} chunks):\")\n",
    "        for i, doc in enumerate(result['source_documents'][:2]):  \n",
    "            print(f\"  [{i+1}] {doc.page_content[:150]}...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
